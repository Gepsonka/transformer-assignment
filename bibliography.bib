@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{CNNfield,
  author  = {Lin, Zhijie and He, Zhaoshui and Wang, Jing and Wang, Xu and Su, Wenqing and Wang, Peitao and Liang, Hao},
  year    = {2025},
  month   = {05},
  pages   = {1-11},
  title   = {Adaptive RoI-aware network for accurate banknote recognition using natural images},
  journal = {Soft Computing},
  doi     = {10.1007/s00500-025-10649-1}
}

@online{heycoachRNN,
  author  = {{HeyCoach.in}},
  title   = {Recurrent Neural Networks (RNNs) and their Applications},
  url     = {https://blog.heycoach.in/recurrent-neural-networks-rnns-and-their-applications/},
  urldate = {2025-06-04},
  year    = {2024}
}

@online{eitcaRNNchallenges,
  author  = {{EITCA (European IT Certification Academy)}},
  title   = {What are the main challenges faced by RNNs during training and how do Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) address these issues?},
  url     = {https://eitca.org/artificial-intelligence/eitc-ai-adl-advanced-deep-learning/recurrent-neural-networks-eitc-ai-adl-advanced-deep-learning/sequences-and-recurrent-networks/examination-review-sequences-and-recurrent-networks/what-are-the-main-challenges-faced-by-rnns-during-training-and-how-do-long-short-term-memory-lstm-networks-and-gated-recurrent-units-grus-address-these-issues/},
  urldate = {2025-06-04},
  year    = {2023}
}

@online{ibmRNN,
  author  = {{IBM}},
  title   = {Recurrent Neural Networks (RNNs)},
  url     = {https://www.ibm.com/think/topics/recurrent-neural-networks},
  urldate = {2025-06-04},
  year    = {2023}
}


@article{ba2016layer,
  title   = {Layer normalization},
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  year    = {2016}
}

@article{dai2024deepseekmoe,
  title   = {Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author  = {Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Yu and others},
  journal = {arXiv preprint arXiv:2401.06066},
  year    = {2024}
}

@online{analyticsvidhya_beam_search,
  author  = {{Analytics Vidhya}},
  title   = {What is Beam Search in NLP Decoding?},
  url     = {https://www.analyticsvidhya.com/blog/2025/01/beam-search-in-nlp-decoding/},
  urldate = {2025-06-04},
  year    = {2025},
  month   = {January}
}

@article{shazeer2017outrageously,
  title   = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author  = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal = {arXiv preprint arXiv:1701.06538},
  year    = {2017}
}

@online{gopubby_deepseekv3,
  author  = {{GoPubby AI}},
  title   = {DeepSeek-V3 Explained (2): DeepSeekMoE},
  url     = {https://ai.gopubby.com/deepseek-v3-explained-2-deepseekmoe-106cffcc56c1},
  urldate = {2025-06-05}
}